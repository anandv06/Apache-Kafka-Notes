1.Data processing started with the idea of capturing and storing data first and then processing it later.

2.When Bigdata processing began to emerge, it followed the same pattern, obtain the data first, saves it, and then, transforms it later. 

3.Data Lake is the result of the same idea. Data Lake is a place where data goes to sit, and after that, you process it in smaller batches. 

4.While many of us are building upon the data lake, at the same time, a whole new fleet of technology companies is architecting their systems in a non-traditional method.

5.They take everything happening in the business and make it available as a stream of events.

6.Then they create applications to tap into the stream, consume data packets in real-time and put them to use for taking critical actions and decision making.  

7.Look  about the change in perspective, we are not looking at the data as a table in a database or a file in the data lake. Data lake is stationary, but the stream is data in motion.

8.When you process a dataset that is already sitting in a database, or it is stored in a data lake, we call it as data processing.

9.But when you are processing a data stream or a dataset which is in motion, we call it "stream processing."

10.And when you want to do it in seconds or in milliseconds, the idea is termed as real-time stream processing. 


10.You already learned and a kind of mastered data processing techniques. It is straightforward. Read data from the database, process it using SQL or using a procedural language, and then, store the outcome back to a database.

11.If you are dealing with big data, the idea is still the same. Read it from the data lake, process it using SQL or using a procedural language, and then, store the outcome back to the data lake.

12.But processing a stream of data is not that straightforward. A stream is a continuous flow of data, it is unbounded. It’s an endless flow that keeps bringing more and more data to your system every second. How do you deal with that? How do you process that stream? Your data is not stored in a database table or in a file. The moment you start thinking about real-time stream processing, a whole bunch of new problems starts popping in your mind. And that’s what this course is all about. Solving those problems. Answering those questions.  
In this course, I am going to teach you two things.

How do you create and manage a stream of data? I mean, to process data from a data lake, you must bring some data into a lake. Similarly, to handle a stream of data, you must create a stream. Right?




1.Stream from kafka
         <null,"Hello hello World">

2.MapValues lowercase   
         <null,"hello hello world">

3.FlatMapValues split by space
         <null,"hello"> <null,"hello"> <null,"world">

4.SelectKey to apply key
         <"hello","hello"> <"hello","hello"> <"world","world">

5.GroupBkey before aggregation
        (<"hello","hello"> <"hello","hello">) (<"world","world">)
            

6.Count occurences in each group
        <"hello",2>  <"world",1>


7.To in order to write the results back to Kafka
        data point is written to kafka

  

Kafka JDBC Connector
======================

create database company;

use company;

create table employee(id int primary key,name text,email text,department text);

insert into employee values(1,'ram','ram@gmail.com','HR');
insert into employee values(2,'rahim','rahim@gmail.com','Insurance');
insert into employee values(3,'david','david@gmail.com','Training');






1.We need 2 dependencies/jar file
   1.mysql connector.jar
   2.kafka connect jdbc

2.Copy both jars inside libs folder of Kafka folder

3.Create a mysql databse and a table

4.Start zookeeper 
  zookeeper-server-start config/zookeeper.properties

5.start kafka
  kafka-server-start config/server.properties

6.start connector

  connect-distrubuted config/connect-distrubuted.properties


POST  http://localhost:8083/connectors -H "Content-Type: application/json" 

-d '{
        "name": "mysql-source-connector",
        "config": {
                "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
                "tasks.max":"2",
                "connection.url": "jdbc:mysql://localhost:3306/company",
                "connection.user": "root",
                "connection.password": "admin",
                "topic.prefix": "pradeep-",
                "mode":"incrementing",
                "table.whitelist":"employees",
                "incrementing.column.name":"id",
                "timestamp.column.name":"modified",
                "poll.interval":1000
                }
        }'


To see the connectors
=====================
http://localhost:8083/connectors
http://localhost:8083/connectors/mysql-source-connector/status





Kafka To MySQL Sink Connector
================================

kafka-topics.bat --zookeeper localhost:2181 --create --topic test --partitions 1 --replication-factor 1



POST  http://localhost:8083/connectors -H "Content-Type: application/json" 


   {
        "name": "mysql-sink-connector",
        "config": {
                "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
                "tasks.max":"1",     
                "connection.url": "jdbc:mysql://localhost:3306/company?user=root&password=admin",
                "topics": "test",
                "auto.create":"true"
                }
     }

http://localhost:8083/connectors
http://localhost:8083/connectors/mysql-source-connector/status



start kafka-console-producer.bat --broker-list localhost:9092 --topic employee
>
{
  "schema": {
    "type": "struct",
    "fields": [
      {
        "type": "int64",
        "optional": false,
        "field": "id"
      },
      {
        "type": "string",
        "optional": true,
        "field": "name"
      },
      {
        "type": "string",
        "optional": true,
        "field": "email"
      },
      {
        "type": "string",
        "optional": true,
        "field": "department"
      }
    ],
    "optional": false,
    "name": "test"
  },
  "payload": {
    "id": 1,
    "name": "ram",
    "email": "ram@gmail.com",
    "department": "training"
}}












